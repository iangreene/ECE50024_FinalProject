# -*- coding: utf-8 -*-
"""ECE50024_FinalProject_Method_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DnK_Osi3Roxwxy3x9WKlYH0tQi7AW-Zt
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import KFold
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Random dataset similar to banana
COLOR = {-1: 'b', 1: 'r'}
MARKER = {-1: '+', 1: 'o'}
FACE = {-1: 'b', 1: 'none'}
SIZE = {-1: 40, 1: 20}

N = 5000

data, labels = make_classification(n_samples=N, n_features=3, n_redundant=1, n_informative=2,n_clusters_per_class=2,flip_y = 0.0,weights= (0.5,0.5))

X_labeled = []
y_labeled = []
for d in zip(labels, data[:, 0], data[:, 1]):
  if d[0] == 0:
      X_labeled.append([d[1],d[2]])
      y_labeled.append(-1)
  else:
      X_labeled.append([d[1],d[2]])
      y_labeled.append(1)

X_labeled = np.array(X_labeled)
y_labeled = np.array(y_labeled)


for label in [-1, 1]:
    mask = (y_labeled == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

# 'Banana' Dataset
df = pd.read_csv("banana.csv")

X_labeled = df.iloc[:, :-1].to_numpy()
y_labeled = df.iloc[:, -1].to_numpy()

for label in [-1, 1]:
    mask = (y_labeled == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

print(X_labeled)
print(np.shape(X_labeled))
print(y_labeled)
print(np.shape(y_labeled))

# Add noise to dataset
noise_rho_1 = 0.4
noise_rho_2 = 0.1

y_noisy = np.copy(y_labeled)
for i in range(len(y_noisy)):
    if y_labeled[i] == 1:
        if np.random.random() < noise_rho_1:
            y_noisy[i] = -1
    elif y_labeled[i] == -1:
        if np.random.random() < noise_rho_2:
            y_noisy[i] = 1

for label in [-1, 1]:
    mask = (y_noisy == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

# Baseline
svm = SVC()
svm.fit(X_labeled, y_noisy)

y_pred = svm.predict(X_labeled)

for label in [-1, 1]:
    mask = (y_noisy == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

for label in [-1, 1]:
    mask = (y_pred == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

acc_train = accuracy_score(y_labeled, y_pred)
print('Training accuracy:', acc_train)

# Implementation of Method of Label-Dependent Costs with Known Noise Rates
rho_plus_1 = noise_rho_1  
rho_minus_1 = noise_rho_2 
alpha_star = (1 - rho_plus_1 + rho_minus_1) / 2
print(alpha_star)

svm = SVC(class_weight={1: 1/(alpha_star), -1: 1/(1-alpha_star)})
svm.fit(X_labeled, y_noisy)

y_pred = svm.predict(X_labeled)
print(accuracy_score(y_labeled, y_pred))

for label in [-1, 1]:
    mask = (y_noisy == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

for label in [-1, 1]:
    mask = (y_pred == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

# Implementation of Method of Label-Dependent Costs with Unknown Noise Rates
k = 5

alpha_star_values = np.linspace(0.1, 0.9, 17)

kf = KFold(n_splits=k)

best_alpha_star = None
best_loss = np.inf

def alpha_weighted_loss(t, y, alpha):
    return (1-alpha) * (y == 1) * (t <= 0) + (alpha) * (y == -1) * (t > 0)


for alpha_star in alpha_star_values:
    losses = []

    for train_index, val_index in kf.split(X_labeled):
        X_train, X_val = X_labeled[train_index], X_labeled[val_index]
        y_train, y_val = y_noisy[train_index], y_noisy[val_index]

        svm = SVC(class_weight={1: 1/(alpha_star), -1: 1/(1-alpha_star)})
        svm.fit(X_train, y_train)

        y_pred = svm.decision_function(X_train)
        loss = np.mean(alpha_weighted_loss(y_pred, y_train, (1-alpha_star)))
        losses.append(loss)

    avg_loss = np.mean(losses)

    if avg_loss < best_loss:
        best_alpha_star = alpha_star
        best_loss = avg_loss

# Train the model with the optimal alpha_star on the entire labeled dataset
svm = SVC(class_weight={1: 1/(best_alpha_star), -1: 1/(1-best_alpha_star)})
svm.fit(X_labeled, y_noisy)

# Predict and evaluate the model
y_pred = svm.predict(X_labeled)
print("Optimal alpha_star:", best_alpha_star)
print("Best cross-validated loss:", best_loss)
print("Accuracy on the entire labeled dataset:", accuracy_score(y_labeled, y_pred))

for label in [-1, 1]:
    mask = (y_noisy == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

for label in [-1, 1]:
    mask = (y_pred == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

# Dataset Creation for UCI Banknote
df = pd.read_csv("data_banknote_authentication.txt", names=["variance", "skewness", "curtosis", "entropy", "class"])

df["class"] = df["class"].replace(0, -1)

X_labeled = df.iloc[:, :-1].to_numpy()
y_labeled = df.iloc[:, -1].to_numpy()

# Adding Noise to UCI Banknote Datset
rho_plus = 0.5
rho_minus = 0.1

y_noisy = np.copy(y_labeled)
for i in range(len(y_noisy)):
    if y_labeled[i] == 1:
        if np.random.random() < rho_plus:
            y_noisy[i] = -1
    elif y_labeled[i] == -1:
        if np.random.random() < rho_minus:
            y_noisy[i] = 1

# Baseline
svm = SVC()
svm.fit(X_labeled, y_noisy)

y_pred = svm.predict(X_labeled)

acc_train = accuracy_score(y_labeled, y_pred)
print('Training accuracy:', acc_train)

# Implementation of Method of Label-Dependent Costs with Known Noise Rates
rho_plus_1 = rho_plus
rho_minus_1 = rho_minus
alpha_star = (1 - rho_plus_1 + rho_minus_1) / 2
print(alpha_star)

svm = SVC(class_weight={1: 1/(alpha_star), -1: 1/(1-alpha_star)})
svm.fit(X_labeled, y_noisy)

y_pred = svm.predict(X_labeled)
print(accuracy_score(y_labeled, y_pred))

# Implementation of Method of Label-Dependent Costs with Unknown Noise Rates
k = 5

alpha_star_values = np.linspace(0.1, 0.9, 17)

kf = KFold(n_splits=k)

best_alpha_star = None
best_loss = np.inf

def alpha_weighted_loss(t, y, alpha):
    return (1-alpha) * (y == 1) * (t <= 0) + (alpha) * (y == -1) * (t > 0)


for alpha_star in alpha_star_values:
    losses = []

    for train_index, val_index in kf.split(X_labeled):
        X_train, X_val = X_labeled[train_index], X_labeled[val_index]
        y_train, y_val = y_noisy[train_index], y_noisy[val_index]

        svm = SVC(class_weight={1: 1/(alpha_star), -1: 1/(1-alpha_star)})
        svm.fit(X_train, y_train)

        y_pred = svm.decision_function(X_train)
        loss = np.mean(alpha_weighted_loss(y_pred, y_train, (1-alpha_star)**3))
        losses.append(loss)

    avg_loss = np.mean(losses)

    if avg_loss < best_loss:
        best_alpha_star = alpha_star
        best_loss = avg_loss

# Train the model with the optimal alpha_star on the entire labeled dataset
svm = SVC(class_weight={1: 1/(best_alpha_star), -1: 1/(1-best_alpha_star)})
svm.fit(X_labeled, y_noisy)

# Predict and evaluate the model
y_pred = svm.predict(X_labeled)
print("Optimal alpha_star:", best_alpha_star)
print("Best cross-validated loss:", best_loss)
print("Accuracy on the entire labeled dataset:", accuracy_score(y_labeled, y_pred))

# Dataset Creation for UCI Heart
df = pd.read_csv("dataR2.csv")
df["Classification"] = df["Classification"].replace(2, -1)

X_labeled = df.iloc[:, :-1].to_numpy()
y_labeled = df.iloc[:, -1].to_numpy()

print(X_labeled)
scaler = MinMaxScaler()
X_labeled = scaler.fit_transform(X_labeled)
print(X_labeled)

# Adding Noise to UCI Heart Datset
rho_plus = 0.1
rho_minus = 0.4

y_noisy = np.copy(y_labeled)
for i in range(len(y_noisy)):
    if y_labeled[i] == 1:
        if np.random.random() < rho_plus:
            y_noisy[i] = -1
    elif y_labeled[i] == -1:
        if np.random.random() < rho_minus:
            y_noisy[i] = 1

# Baseline
svm = SVC()
svm.fit(X_labeled, y_noisy)

y_pred = svm.predict(X_labeled)

acc_train = accuracy_score(y_labeled, y_pred)
print('Training accuracy:', acc_train)

# Implementation of Method of Label-Dependent Costs with Known Noise Rates
rho_plus_1 = rho_plus
rho_minus_1 = rho_minus
alpha_star = (1 - rho_plus_1 + rho_minus_1) / 2
print(alpha_star)

svm = SVC(class_weight={1: 1/(alpha_star), -1: 1/(1-alpha_star)})
svm.fit(X_labeled, y_noisy)

y_pred = svm.predict(X_labeled)
print(accuracy_score(y_labeled, y_pred))

# Implementation of Method of Label-Dependent Costs with Unknown Noise Rates
k = 5

alpha_star_values = np.linspace(0.1, 0.9, 17)

kf = KFold(n_splits=k)

best_alpha_star = None
best_loss = np.inf

def alpha_weighted_loss(t, y, alpha):
    return (1-alpha) * (y == 1) * (t <= 0) + (alpha) * (y == -1) * (t > 0)


for alpha_star in alpha_star_values:
    losses = []

    for train_index, val_index in kf.split(X_labeled):
        X_train, X_val = X_labeled[train_index], X_labeled[val_index]
        y_train, y_val = y_noisy[train_index], y_noisy[val_index]

        svm = SVC(class_weight={1: 1/(alpha_star), -1: 1/(1-alpha_star)})
        svm.fit(X_train, y_train)

        y_pred = svm.decision_function(X_train)
        loss = np.mean(alpha_weighted_loss(y_pred, y_train, (1+alpha_star)**2*(1-alpha_star)))
        losses.append(loss)

    avg_loss = np.mean(losses)

    if avg_loss < best_loss:
        best_alpha_star = alpha_star
        best_loss = avg_loss

# Train the model with the optimal alpha_star on the entire labeled dataset
svm = SVC(class_weight={1: 1/(best_alpha_star), -1: 1/(1-best_alpha_star)})
svm.fit(X_labeled, y_noisy)

# Predict and evaluate the model
y_pred = svm.predict(X_labeled)
print("Optimal alpha_star:", best_alpha_star)
print("Best cross-validated loss:", best_loss)
print("Accuracy on the entire labeled dataset:", accuracy_score(y_labeled, y_pred))