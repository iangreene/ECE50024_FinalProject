# -*- coding: utf-8 -*-
"""ECE50024_FinalProject_Method_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vMOQxPS_eq4EwccTyeZj7IUXfA9iqZR3
"""

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from tqdm import tqdm
import concurrent.futures
import warnings

# Dataset Creation and Plotting
COLOR = {-1: 'b', 1: 'r'}
MARKER = {-1: 'o', 1: '+'}
FACE = {-1: 'none', 1: 'r'}
SIZE = {-1: 20, 1: 40}

N = 2500
buffer_size = 100

np.random.seed(0)
X = np.random.uniform(low=-buffer_size, high=buffer_size, size=(N, 2))

margin = -0.15*buffer_size
buffer_zone = 0.3*buffer_size

y = np.zeros(N)
for i in range(N):
    if X[i, 0] <= X[i, 1] - margin - buffer_zone:
        y[i] = -1
    elif X[i, 0] >= X[i, 1] - margin + buffer_zone:
        y[i] = 1

X_labeled = X[y != 0]
y_labeled = y[y != 0]

for label in [-1, 1]:
    mask = (y_labeled == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

# Making Data Noisy and Plotting
rho_plus = 0.2
rho_minus = 0.4

y_noisy = np.copy(y_labeled)
for i in range(len(y_noisy)):
    if y_labeled[i] == 1:
        if np.random.random() < rho_plus:
            y_noisy[i] = -1
    elif y_labeled[i] == -1:
        if np.random.random() < rho_minus:
            y_noisy[i] = 1

for label in [-1, 1]:
    mask = (y_noisy == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

# Implementation of Method of Unbiased Estimators with Known Noise
def logistic_loss(t, y):
    return np.log(1 + np.exp(-t * y))

def unbiased_loss(t, y, rho_plus, rho_minus):
    rho_y = (y + 1) / 2 * (rho_plus - rho_minus) + rho_minus
    rho_minus_y = (y + 1) / 2 * (rho_minus - rho_plus) + rho_plus
    return ((1 - rho_minus_y) * logistic_loss(t, y) - rho_y * logistic_loss(t, -y)) / (1 - rho_plus - rho_minus)

def average_loss(params, X, y, rho_plus, rho_minus):
    t = np.dot(X, params)
    unbiased_losses = unbiased_loss(t, y, rho_plus, rho_minus)
    return np.mean(unbiased_losses)

def find_best_params(X, y, rho_plus, rho_minus):
    init_params = np.zeros(X.shape[1])
    result = minimize(average_loss, init_params, args=(X, y, rho_plus, rho_minus))
    return result.x

best_params = find_best_params(X_labeled, y_noisy, rho_plus, rho_minus)

print(best_params)

def predict(X, params):
    scores = np.dot(X, params)
    return np.sign(scores)

y_pred = predict(X_labeled, best_params)

# Plotting and Accuracy for Method of Unbiased Estimators with Known Noise Rates
print(accuracy_score(y_labeled, y_pred))

for label in [-1, 1]:
    mask = (y_noisy == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

for label in [-1, 1]:
    mask = (y_pred == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

# Baseline using normal Logistic Regression
clf = LogisticRegression()
clf.fit(X_labeled, y_noisy)

y_pred = clf.predict(X_labeled)

acc = accuracy_score(y_labeled, y_pred)
print('Accuracy:', acc)

for label in [-1, 1]:
    mask = (y_noisy == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

for label in [-1, 1]:
    mask = (y_pred == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

# Implementation of Method of Unbiased Estimators with Unknown Noise Rates
warnings.filterwarnings("ignore")

def logistic_loss(t, y):
    return np.log(1 + np.exp(-t * y))

def unbiased_loss(t, y, rho_plus, rho_minus):
    rho_y = (y + 1) / 2 * (rho_plus - rho_minus) + rho_minus
    rho_minus_y = (y + 1) / 2 * (rho_minus - rho_plus) + rho_plus
    return ((1 - rho_minus_y) * logistic_loss(t, y) - rho_y * logistic_loss(t, -y)) / (1 - rho_plus - rho_minus)

def average_loss(params, X, y, rho_plus, rho_minus):
    t = np.dot(X, params)
    unbiased_losses = unbiased_loss(t, y, rho_plus, rho_minus)
    return np.mean(unbiased_losses)

def find_best_params(X, y, rho_plus, rho_minus):
    init_params = np.zeros(X.shape[1])
    result = minimize(average_loss, init_params, args=(X, y, rho_plus, rho_minus))
    return result.x

def single_fold(train_index, test_index, X, y, rho_plus, rho_minus):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    best_params = find_best_params(X_train, y_train, rho_plus, rho_minus)
    fold_loss = average_loss(best_params, X_train, y_train, rho_plus, rho_minus)
    return fold_loss

def k_fold_cross_validation(X, y, rho_plus, rho_minus, k=5):
    kf = KFold(n_splits=k)
    fold_results = []

    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(single_fold, train_index, test_index, X, y, rho_plus, rho_minus)
                   for train_index, test_index in kf.split(X)]
        fold_results = [f.result() for f in futures]

    return np.mean(fold_results)

def optimize_rho(X, y, rho_values, k=5):
    best_rho_plus, best_rho_minus = None, None
    best_loss = np.inf
    
    for rho_plus in tqdm(rho_values, desc='Optimizing rho_plus'):
        for rho_minus in tqdm(rho_values, desc='Optimizing rho_minus', leave=False):
            if rho_plus + rho_minus >= 1:
                continue
            
            mean_loss = k_fold_cross_validation(X, y, rho_plus, rho_minus, k)
            if mean_loss < best_loss:
                best_loss = mean_loss
                best_rho_plus, best_rho_minus = rho_plus, rho_minus

    best_params = find_best_params(X, y, best_rho_plus, best_rho_minus)
    return best_rho_plus, best_rho_minus, best_loss, best_params

rho_values = np.linspace(0, 0.5, 11)  # Example: 11 values from 0 to 0.5

best_rho_plus, best_rho_minus, best_loss, best_params = optimize_rho(X_labeled, y_noisy, rho_values)
print("")
print("Best rho_plus:", best_rho_plus)
print("Best rho_minus:", best_rho_minus)
print("Best mean loss:", best_loss)
print("Best params:", best_params)
warnings.filterwarnings("default")

def predict(X, params):
    scores = np.dot(X, params)
    return np.sign(scores)

y_pred = predict(X_labeled, best_params)

# Plotting and Accuracy for Method of Unbiased Estimators with Known Noise Rates
print(accuracy_score(y_labeled, y_pred))

for label in [-1, 1]:
    mask = (y_noisy == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

for label in [-1, 1]:
    mask = (y_pred == label)
    plt.scatter(X_labeled[mask, 0], X_labeled[mask, 1], s=SIZE[label], facecolors=FACE[label], edgecolors=COLOR[label], marker=MARKER[label])
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

# Dataset Creation for UCI Banknote
df = pd.read_csv("data_banknote_authentication.txt", names=["variance", "skewness", "curtosis", "entropy", "class"])

df["class"] = df["class"].replace(0, -1)

X_labeled = df.iloc[:, :-1].to_numpy()
y_labeled = df.iloc[:, -1].to_numpy()

# Adding Noise to UCI Banknote Datset
rho_plus = 0.5
rho_minus = 0.1

y_noisy = np.copy(y_labeled)
for i in range(len(y_noisy)):
    if y_labeled[i] == 1:
        if np.random.random() < rho_plus:
            y_noisy[i] = -1
    elif y_labeled[i] == -1:
        if np.random.random() < rho_minus:
            y_noisy[i] = 1

# Implementation of Method of Unbiased Estimators with Known Noise Rates
def logistic_loss(t, y):
    return np.log(1 + np.exp(-t * y))

def unbiased_loss(t, y, rho_plus, rho_minus):
    rho_y = (y + 1) / 2 * (rho_plus - rho_minus) + rho_minus
    rho_minus_y = (y + 1) / 2 * (rho_minus - rho_plus) + rho_plus
    return ((1 - rho_minus_y) * logistic_loss(t, y) - rho_y * logistic_loss(t, -y)) / (1 - rho_plus - rho_minus)

def average_loss(params, X, y, rho_plus, rho_minus):
    t = np.dot(X, params)
    unbiased_losses = unbiased_loss(t, y, rho_plus, rho_minus)
    return np.mean(unbiased_losses)

def find_best_params(X, y, rho_plus, rho_minus):
    init_params = np.zeros(X.shape[1])
    result = minimize(average_loss, init_params, args=(X, y, rho_plus, rho_minus))
    return result.x

best_params = find_best_params(X_labeled, y_noisy, rho_plus, rho_minus)

def predict(X, params):
    scores = np.dot(X, params)
    return np.sign(scores)

y_pred = predict(X_labeled, best_params)

print(accuracy_score(y_labeled, y_pred))

# Implementation of Method of Unbiased Estimators with Unknown Noise Rates
warnings.filterwarnings("ignore")

def logistic_loss(t, y):
    return np.log(1 + np.exp(-t * y))

def unbiased_loss(t, y, rho_plus, rho_minus):
    rho_y = (y + 1) / 2 * (rho_plus - rho_minus) + rho_minus
    rho_minus_y = (y + 1) / 2 * (rho_minus - rho_plus) + rho_plus
    return ((1 - rho_minus_y) * logistic_loss(t, y) - rho_y * logistic_loss(t, -y)) / (1 - rho_plus - rho_minus)

def average_loss(params, X, y, rho_plus, rho_minus):
    t = np.dot(X, params)
    unbiased_losses = unbiased_loss(t, y, rho_plus, rho_minus)
    return np.mean(unbiased_losses)

def find_best_params(X, y, rho_plus, rho_minus):
    init_params = np.zeros(X.shape[1])
    result = minimize(average_loss, init_params, args=(X, y, rho_plus, rho_minus))
    return result.x

def single_fold(train_index, test_index, X, y, rho_plus, rho_minus):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    best_params = find_best_params(X_train, y_train, rho_plus, rho_minus)
    fold_loss = average_loss(best_params, X_train, y_train, rho_plus, rho_minus)
    return fold_loss

def k_fold_cross_validation(X, y, rho_plus, rho_minus, k=5):
    kf = KFold(n_splits=k)
    fold_results = []

    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(single_fold, train_index, test_index, X, y, rho_plus, rho_minus)
                   for train_index, test_index in kf.split(X)]
        fold_results = [f.result() for f in futures]

    return np.mean(fold_results)

def optimize_rho(X, y, rho_values, k=5):
    best_rho_plus, best_rho_minus = None, None
    best_loss = np.inf
    
    for rho_plus in tqdm(rho_values, desc='Optimizing rho_plus'):
        for rho_minus in tqdm(rho_values, desc='Optimizing rho_minus', leave=False):
            if rho_plus + rho_minus >= 1:
                continue
            
            mean_loss = k_fold_cross_validation(X, y, rho_plus, rho_minus, k)
            if mean_loss < best_loss:
                best_loss = mean_loss
                best_rho_plus, best_rho_minus = rho_plus, rho_minus

    best_params = find_best_params(X, y, best_rho_plus, best_rho_minus)
    return best_rho_plus, best_rho_minus, best_loss, best_params

rho_values = np.linspace(0, 0.5, 11)  # Example: 11 values from 0 to 0.5

best_rho_plus, best_rho_minus, best_loss, best_params = optimize_rho(X_labeled, y_noisy, rho_values)
print("")
print("Best rho_plus:", best_rho_plus)
print("Best rho_minus:", best_rho_minus)
print("Best mean loss:", best_loss)
print("Best params:", best_params)
warnings.filterwarnings("default")

def predict(X, params):
    scores = np.dot(X, params)
    return np.sign(scores)

y_pred = predict(X_labeled, best_params)

print(accuracy_score(y_labeled, y_pred))

# Baseline using normal Logistic Regression
clf = LogisticRegression()
clf.fit(X_labeled, y_noisy)

y_pred = clf.predict(X_labeled)

acc = accuracy_score(y_labeled, y_pred)
print('Accuracy:', acc)

# Dataset Creation for UCI Heart
df = pd.read_csv("dataR2.csv")
df["Classification"] = df["Classification"].replace(2, -1)

X_labeled = df.iloc[:, :-1].to_numpy()
y_labeled = df.iloc[:, -1].to_numpy()

print(X_labeled)
scaler = MinMaxScaler()
X_labeled = scaler.fit_transform(X_labeled)
print(X_labeled)

# Adding Noise to UCI Heart Datset
rho_plus = 0.1
rho_minus = 0.4

y_noisy = np.copy(y_labeled)
for i in range(len(y_noisy)):
    if y_labeled[i] == 1:
        if np.random.random() < rho_plus:
            y_noisy[i] = -1
    elif y_labeled[i] == -1:
        if np.random.random() < rho_minus:
            y_noisy[i] = 1

# Implementation of Method of Unbiased Estimators with Known Noise Rates
def logistic_loss(t, y):
    return np.log(1 + np.exp(-t * y))

def unbiased_loss(t, y, rho_plus, rho_minus):
    rho_y = (y + 1) / 2 * (rho_plus - rho_minus) + rho_minus
    rho_minus_y = (y + 1) / 2 * (rho_minus - rho_plus) + rho_plus
    return ((1 - rho_minus_y) * logistic_loss(t, y) - rho_y * logistic_loss(t, -y)) / (1 - rho_plus - rho_minus)

def average_loss(params, X, y, rho_plus, rho_minus):
    t = np.dot(X, params)
    unbiased_losses = unbiased_loss(t, y, rho_plus, rho_minus)
    return np.mean(unbiased_losses)

def find_best_params(X, y, rho_plus, rho_minus):
    init_params = np.zeros(X.shape[1])
    result = minimize(average_loss, init_params, args=(X, y, rho_plus, rho_minus))
    return result.x

best_params = find_best_params(X_labeled, y_noisy, rho_plus, rho_minus)

def predict(X, params):
    scores = np.dot(X, params)
    return np.sign(scores)

y_pred = predict(X_labeled, best_params)

print(accuracy_score(y_labeled, y_pred))

# Implementation of Method of Unbiased Estimators with Unknown Noise Rates
warnings.filterwarnings("ignore")

def logistic_loss(t, y):
    return np.log(1 + np.exp(-t * y))

def unbiased_loss(t, y, rho_plus, rho_minus):
    rho_y = (y + 1) / 2 * (rho_plus - rho_minus) + rho_minus
    rho_minus_y = (y + 1) / 2 * (rho_minus - rho_plus) + rho_plus
    return ((1 - rho_minus_y) * logistic_loss(t, y) - rho_y * logistic_loss(t, -y)) / (1 - rho_plus - rho_minus)

def average_loss(params, X, y, rho_plus, rho_minus):
    t = np.dot(X, params)
    unbiased_losses = unbiased_loss(t, y, rho_plus, rho_minus)
    return np.mean(unbiased_losses)

def find_best_params(X, y, rho_plus, rho_minus):
    init_params = np.zeros(X.shape[1])
    result = minimize(average_loss, init_params, args=(X, y, rho_plus, rho_minus))
    return result.x

def single_fold(train_index, test_index, X, y, rho_plus, rho_minus):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    best_params = find_best_params(X_train, y_train, rho_plus, rho_minus)
    fold_loss = average_loss(best_params, X_train, y_train, rho_plus, rho_minus)
    return fold_loss

def k_fold_cross_validation(X, y, rho_plus, rho_minus, k=5):
    kf = KFold(n_splits=k)
    fold_results = []

    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(single_fold, train_index, test_index, X, y, rho_plus, rho_minus)
                   for train_index, test_index in kf.split(X)]
        fold_results = [f.result() for f in futures]

    return np.mean(fold_results)

def optimize_rho(X, y, rho_values, k=5):
    best_rho_plus, best_rho_minus = None, None
    best_loss = np.inf
    
    for rho_plus in tqdm(rho_values, desc='Optimizing rho_plus'):
        for rho_minus in tqdm(rho_values, desc='Optimizing rho_minus', leave=False):
            if rho_plus + rho_minus >= 1:
                continue
            
            mean_loss = k_fold_cross_validation(X, y, rho_plus, rho_minus, k)
            if mean_loss < best_loss:
                best_loss = mean_loss
                best_rho_plus, best_rho_minus = rho_plus, rho_minus

    best_params = find_best_params(X, y, best_rho_plus, best_rho_minus)
    return best_rho_plus, best_rho_minus, best_loss, best_params

rho_values = np.linspace(0, 0.5, 11)  # Example: 11 values from 0 to 0.5

best_rho_plus, best_rho_minus, best_loss, best_params = optimize_rho(X_labeled, y_noisy, rho_values)
print("")
print("Best rho_plus:", best_rho_plus)
print("Best rho_minus:", best_rho_minus)
print("Best mean loss:", best_loss)
print("Best params:", best_params)
warnings.filterwarnings("default")

def predict(X, params):
    scores = np.dot(X, params)
    return np.sign(scores)

y_pred = predict(X_labeled, best_params)

print(accuracy_score(y_labeled, y_pred))

# Baseline using normal Logistic Regression
clf = LogisticRegression()
clf.fit(X_labeled, y_noisy)

y_pred = clf.predict(X_labeled)

acc = accuracy_score(y_labeled, y_pred)
print('Accuracy:', acc)